{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TEXT PRE PROCESSING"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('Restaurant_Reviews.tsv',delimiter='\\t',quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Service was very prompt.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Would not go back.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The cashier had no care what so ever on what I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I tried the Cape Cod ravoli, chicken, with cra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I was disgusted because I was pretty sure that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I was shocked because no signs indicate cash o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Highly recommended.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Waitress was a little slow in service.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This place is not worth your time, let alone V...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>did not like at all.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The Burrittos Blah!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The food, amazing.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Service is also cute.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I could care less... The interior is just beau...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>So they performed.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>That's right....the red velvet cake.....ohhh t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>- They never brought a salad we asked for.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>This hole in the wall has great Mexican street...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Took an hour to get our food only 4 tables in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The worst was the salmon sashimi.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>I immediately said I wanted to talk to the man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>The ambiance isn't much better.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Unfortunately, it only set us up for disapppoi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>The food wasn't good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>Your servers suck, wait, correction, our serve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>What happened next was pretty....off putting.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>too bad cause I know it's family owned, I real...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Overpriced for what you are getting.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>I vomited in the bathroom mid lunch.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>I kept looking at the time and it had soon bec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>I have been to very few places to eat that und...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>We started with the tuna sashimi which was bro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>Food was below average.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>It sure does beat the nachos at the movies but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>All in all, Ha Long Bay was a bit of a flop.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>The problem I have is that they charge $11.99 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>Shrimp- When I unwrapped it (I live only 1/2 a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>It lacked flavor, seemed undercooked, and dry.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>It really is impressive that the place hasn't ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>I would avoid this place if you are staying in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>The refried beans that came with my meal were ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Spend your money and time some place else.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>A lady at the table next to us found a live gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>the presentation of the food was awful.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>I can't tell you how disappointed I was.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review  Liked\n",
       "0                             Wow... Loved this place.      1\n",
       "1                                   Crust is not good.      0\n",
       "2            Not tasty and the texture was just nasty.      0\n",
       "3    Stopped by during the late May bank holiday of...      1\n",
       "4    The selection on the menu was great and so wer...      1\n",
       "5       Now I am getting angry and I want my damn pho.      0\n",
       "6                Honeslty it didn't taste THAT fresh.)      0\n",
       "7    The potatoes were like rubber and you could te...      0\n",
       "8                            The fries were great too.      1\n",
       "9                                       A great touch.      1\n",
       "10                            Service was very prompt.      1\n",
       "11                                  Would not go back.      0\n",
       "12   The cashier had no care what so ever on what I...      0\n",
       "13   I tried the Cape Cod ravoli, chicken, with cra...      1\n",
       "14   I was disgusted because I was pretty sure that...      0\n",
       "15   I was shocked because no signs indicate cash o...      0\n",
       "16                                 Highly recommended.      1\n",
       "17              Waitress was a little slow in service.      0\n",
       "18   This place is not worth your time, let alone V...      0\n",
       "19                                did not like at all.      0\n",
       "20                                 The Burrittos Blah!      0\n",
       "21                                  The food, amazing.      1\n",
       "22                               Service is also cute.      1\n",
       "23   I could care less... The interior is just beau...      1\n",
       "24                                  So they performed.      1\n",
       "25   That's right....the red velvet cake.....ohhh t...      1\n",
       "26          - They never brought a salad we asked for.      0\n",
       "27   This hole in the wall has great Mexican street...      1\n",
       "28   Took an hour to get our food only 4 tables in ...      0\n",
       "29                   The worst was the salmon sashimi.      0\n",
       "..                                                 ...    ...\n",
       "970  I immediately said I wanted to talk to the man...      0\n",
       "971                    The ambiance isn't much better.      0\n",
       "972  Unfortunately, it only set us up for disapppoi...      0\n",
       "973                              The food wasn't good.      0\n",
       "974  Your servers suck, wait, correction, our serve...      0\n",
       "975      What happened next was pretty....off putting.      0\n",
       "976  too bad cause I know it's family owned, I real...      0\n",
       "977               Overpriced for what you are getting.      0\n",
       "978               I vomited in the bathroom mid lunch.      0\n",
       "979  I kept looking at the time and it had soon bec...      0\n",
       "980  I have been to very few places to eat that und...      0\n",
       "981  We started with the tuna sashimi which was bro...      0\n",
       "982                            Food was below average.      0\n",
       "983  It sure does beat the nachos at the movies but...      0\n",
       "984       All in all, Ha Long Bay was a bit of a flop.      0\n",
       "985  The problem I have is that they charge $11.99 ...      0\n",
       "986  Shrimp- When I unwrapped it (I live only 1/2 a...      0\n",
       "987     It lacked flavor, seemed undercooked, and dry.      0\n",
       "988  It really is impressive that the place hasn't ...      0\n",
       "989  I would avoid this place if you are staying in...      0\n",
       "990  The refried beans that came with my meal were ...      0\n",
       "991         Spend your money and time some place else.      0\n",
       "992  A lady at the table next to us found a live gr...      0\n",
       "993            the presentation of the food was awful.      0\n",
       "994           I can't tell you how disappointed I was.      0\n",
       "995  I think food should have flavor and texture an...      0\n",
       "996                           Appetite instantly gone.      0\n",
       "997  Overall I was not impressed and would not go b...      0\n",
       "998  The whole experience was underwhelming, and I ...      0\n",
       "999  Then, as if I hadn't wasted enough of my life ...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#removing punctuation,numbers to stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer#library used for stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "data=[]\n",
    "for i in range(0,1000):\n",
    "    review=dataset[\"Review\"][i]\n",
    "    review=re.sub('[^a-zA-Z]',' ',review)#removing punctuation,numbers\n",
    "    review=review.lower()#converts each word to lower\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if not word in set(stopwords.words('english'))]#stemming\n",
    "    review=' '.join(review)\n",
    "    data.append(review)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#splitting the data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.feature_extraction.text import CountVectorizer\n",
    "cv= CountVectorizer(max_features=1500)\n",
    "x= cv.fit_transform(data).toarray()\n",
    "y= dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MODEL BUILDING"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#importing model building libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#adding input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1500, activation=\"sigmoid\", units=1000, kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(input_dim=1500,init=\"random_uniform\",activation=\"sigmoid\",output_dim=1000))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#adding hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=100, kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(init=\"random_uniform\",activation=\"sigmoid\",output_dim=100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#adding output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(output_dim=1,init='random_uniform',activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#configure the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0231 - acc: 0.9886\n",
      "Epoch 2/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.9914A: 0s - loss: 0.0221 - \n",
      "Epoch 3/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0330 - acc: 0.9871\n",
      "Epoch 4/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.9886\n",
      "Epoch 5/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0205 - acc: 0.9900A: 0s - loss: 0.0174 - ac\n",
      "Epoch 6/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.9914\n",
      "Epoch 7/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0233 - acc: 0.9900\n",
      "Epoch 8/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.9886A: 0s - loss: 0.0194 - ac - ETA: 0s - loss: 0.0177 - acc:\n",
      "Epoch 9/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.9900\n",
      "Epoch 10/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9900\n",
      "Epoch 11/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.9900\n",
      "Epoch 12/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0199 - acc: 0.9929\n",
      "Epoch 13/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.9929\n",
      "Epoch 14/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0218 - acc: 0.9886\n",
      "Epoch 15/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0208 - acc: 0.9871\n",
      "Epoch 16/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.9871\n",
      "Epoch 17/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0198 - acc: 0.9871\n",
      "Epoch 18/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.9914\n",
      "Epoch 19/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.9886\n",
      "Epoch 20/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0235 - acc: 0.9900A: 0s - loss: 0.0268 - acc: 0.9 - ETA: 0s - loss: 0.0225 - acc: 0\n",
      "Epoch 21/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.9943\n",
      "Epoch 22/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0259 - acc: 0.9871\n",
      "Epoch 23/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0211 - acc: 0.9914\n",
      "Epoch 24/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.9900\n",
      "Epoch 25/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.9886A: 0s - loss: 0.0239 - acc: 0.9\n",
      "Epoch 26/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.9900A: 0s - loss: 0.0197 - acc: 0.991\n",
      "Epoch 27/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.9886\n",
      "Epoch 28/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.9886A: 0s - loss: 0.0214 - acc: 0.98 - ETA: 0s - loss: 0.0241 - a\n",
      "Epoch 29/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0236 - acc: 0.9914\n",
      "Epoch 30/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.9900\n",
      "Epoch 31/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0199 - acc: 0.9886\n",
      "Epoch 32/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.9914\n",
      "Epoch 33/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0308 - acc: 0.9871\n",
      "Epoch 34/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0270 - acc: 0.9914\n",
      "Epoch 35/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.9886\n",
      "Epoch 36/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0241 - acc: 0.9914\n",
      "Epoch 37/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.9914\n",
      "Epoch 38/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0197 - acc: 0.9914\n",
      "Epoch 39/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.9900\n",
      "Epoch 40/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.9900\n",
      "Epoch 41/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0220 - acc: 0.9900\n",
      "Epoch 42/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.9914\n",
      "Epoch 43/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.9871\n",
      "Epoch 44/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.9914\n",
      "Epoch 45/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.9886\n",
      "Epoch 46/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0205 - acc: 0.9900\n",
      "Epoch 47/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.9900\n",
      "Epoch 48/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0218 - acc: 0.9886\n",
      "Epoch 49/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.9914\n",
      "Epoch 50/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.9914A: 0s - loss: 0.0239 - acc: \n",
      "Epoch 51/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0262 - acc: 0.9900\n",
      "Epoch 52/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.9900\n",
      "Epoch 53/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.9886\n",
      "Epoch 54/200\n",
      "700/700 [==============================] - ETA: 0s - loss: 0.0195 - acc: 0.989 - 1s 2ms/step - loss: 0.0187 - acc: 0.9900\n",
      "Epoch 55/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.9914\n",
      "Epoch 56/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.9900A: 0s - loss: 0.0184 - acc\n",
      "Epoch 57/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.9929\n",
      "Epoch 58/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.9914\n",
      "Epoch 59/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.9886\n",
      "Epoch 60/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.9871\n",
      "Epoch 61/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.9886\n",
      "Epoch 62/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.9900\n",
      "Epoch 63/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.9900\n",
      "Epoch 64/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.9886\n",
      "Epoch 65/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.9914\n",
      "Epoch 66/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9886\n",
      "Epoch 67/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.9900\n",
      "Epoch 68/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.9886\n",
      "Epoch 69/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.9914A: 0s - loss: 0.0213 - acc\n",
      "Epoch 70/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.9914\n",
      "Epoch 71/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.9914\n",
      "Epoch 72/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.9900A: 1s - loss: \n",
      "Epoch 73/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0238 - acc: 0.9886A: 0s - loss: 0.0247 - acc: 0.988\n",
      "Epoch 74/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.9929\n",
      "Epoch 75/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.9886A: 1s - loss: 0.032 - ETA: 0s - loss: 0.0230 - acc: 0.\n",
      "Epoch 76/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.9900\n",
      "Epoch 77/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9900\n",
      "Epoch 78/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.9914A: 0s - loss: 0.0215 - acc: 0.9\n",
      "Epoch 79/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.9900\n",
      "Epoch 80/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.9914\n",
      "Epoch 81/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.9900\n",
      "Epoch 82/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.9914\n",
      "Epoch 83/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.9886\n",
      "Epoch 84/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.9914\n",
      "Epoch 85/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0241 - acc: 0.9886\n",
      "Epoch 86/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.9900\n",
      "Epoch 87/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.9900\n",
      "Epoch 88/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.9900\n",
      "Epoch 89/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.9900\n",
      "Epoch 90/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.9886A: 0s - loss: 0.0133 - acc: 0.98 - ETA: 0s - loss: 0.0117 -\n",
      "Epoch 91/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.9914\n",
      "Epoch 92/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0178 - acc: 0.9886\n",
      "Epoch 93/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.9914\n",
      "Epoch 94/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.9914\n",
      "Epoch 95/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.9900\n",
      "Epoch 96/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.9900\n",
      "Epoch 97/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0185 - acc: 0.9900\n",
      "Epoch 98/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0208 - acc: 0.9914\n",
      "Epoch 99/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0193 - acc: 0.9900\n",
      "Epoch 100/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.9914\n",
      "Epoch 101/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0183 - acc: 0.9886\n",
      "Epoch 102/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0173 - acc: 0.9900\n",
      "Epoch 103/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.9900\n",
      "Epoch 104/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.9900\n",
      "Epoch 105/200\n",
      "700/700 [==============================] - ETA: 0s - loss: 0.0163 - acc: 0.992 - 1s 2ms/step - loss: 0.0180 - acc: 0.9914\n",
      "Epoch 106/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.9900\n",
      "Epoch 107/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0184 - acc: 0.9914\n",
      "Epoch 108/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0192 - acc: 0.9886\n",
      "Epoch 109/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.9914\n",
      "Epoch 110/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.9871\n",
      "Epoch 111/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0196 - acc: 0.9914\n",
      "Epoch 112/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.9900\n",
      "Epoch 113/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0241 - acc: 0.9857\n",
      "Epoch 114/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0269 - acc: 0.9857\n",
      "Epoch 115/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0180 - acc: 0.9914\n",
      "Epoch 116/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0183 - acc: 0.9914\n",
      "Epoch 117/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0169 - acc: 0.9914\n",
      "Epoch 118/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0178 - acc: 0.9914\n",
      "Epoch 119/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0177 - acc: 0.9900\n",
      "Epoch 120/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0187 - acc: 0.9886\n",
      "Epoch 121/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0192 - acc: 0.9914\n",
      "Epoch 122/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0175 - acc: 0.9914A: 0s - loss: 0.0187 - acc: 0\n",
      "Epoch 123/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0189 - acc: 0.9886\n",
      "Epoch 124/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0194 - acc: 0.9900\n",
      "Epoch 125/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0181 - acc: 0.9900\n",
      "Epoch 126/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0170 - acc: 0.9914\n",
      "Epoch 127/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0183 - acc: 0.9914\n",
      "Epoch 128/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0171 - acc: 0.9900\n",
      "Epoch 129/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0186 - acc: 0.9914\n",
      "Epoch 130/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0185 - acc: 0.9914\n",
      "Epoch 131/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0177 - acc: 0.9914\n",
      "Epoch 132/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0172 - acc: 0.9900\n",
      "Epoch 133/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0191 - acc: 0.9914A: 0s - loss: 0.0162 - ac\n",
      "Epoch 134/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0180 - acc: 0.9900A: 1s - loss: \n",
      "Epoch 135/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0173 - acc: 0.9900\n",
      "Epoch 136/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0173 - acc: 0.9914\n",
      "Epoch 137/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0178 - acc: 0.9914\n",
      "Epoch 138/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0180 - acc: 0.9914\n",
      "Epoch 139/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0171 - acc: 0.9914\n",
      "Epoch 140/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0186 - acc: 0.9900\n",
      "Epoch 141/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.9914\n",
      "Epoch 142/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.9900\n",
      "Epoch 143/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 0.9900\n",
      "Epoch 144/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9900\n",
      "Epoch 145/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.9900\n",
      "Epoch 146/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0176 - acc: 0.9914\n",
      "Epoch 147/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.9886\n",
      "Epoch 148/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0178 - acc: 0.9914\n",
      "Epoch 149/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.9914\n",
      "Epoch 150/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.9914\n",
      "Epoch 151/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.9871\n",
      "Epoch 152/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.9900A: 1s - loss: 0.\n",
      "Epoch 153/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.9914\n",
      "Epoch 154/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.9900\n",
      "Epoch 155/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.9914\n",
      "Epoch 156/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.9900\n",
      "Epoch 157/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.9900\n",
      "Epoch 158/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 0.9900\n",
      "Epoch 159/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.9914\n",
      "Epoch 160/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.9900\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.9929\n",
      "Epoch 162/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.9871A: 0s - loss: 0.0211 - acc: 0.988\n",
      "Epoch 163/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.9914\n",
      "Epoch 164/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.9886\n",
      "Epoch 165/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0182 - acc: 0.9914\n",
      "Epoch 166/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0170 - acc: 0.9914\n",
      "Epoch 167/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.9914\n",
      "Epoch 168/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.9914\n",
      "Epoch 169/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.9914\n",
      "Epoch 170/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0196 - acc: 0.9900\n",
      "Epoch 171/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.9914\n",
      "Epoch 172/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.9914\n",
      "Epoch 173/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0169 - acc: 0.9914\n",
      "Epoch 174/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.9914A: 0s - loss: 0.0129\n",
      "Epoch 175/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0169 - acc: 0.9914\n",
      "Epoch 176/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.9900\n",
      "Epoch 177/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 0.9900\n",
      "Epoch 178/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0167 - acc: 0.9914\n",
      "Epoch 179/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 0.9900\n",
      "Epoch 180/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.9914\n",
      "Epoch 181/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.9914\n",
      "Epoch 182/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0197 - acc: 0.9929\n",
      "Epoch 183/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0182 - acc: 0.9900\n",
      "Epoch 184/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0166 - acc: 0.9914\n",
      "Epoch 185/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0169 - acc: 0.9914\n",
      "Epoch 186/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0168 - acc: 0.9914\n",
      "Epoch 187/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0166 - acc: 0.9929\n",
      "Epoch 188/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0167 - acc: 0.9914\n",
      "Epoch 189/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0167 - acc: 0.9900\n",
      "Epoch 190/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.9914\n",
      "Epoch 191/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.9900\n",
      "Epoch 192/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 0.9914\n",
      "Epoch 193/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0167 - acc: 0.9914A: 0s - loss: 0.0198 \n",
      "Epoch 194/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.9900\n",
      "Epoch 195/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0165 - acc: 0.9914\n",
      "Epoch 196/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.9914\n",
      "Epoch 197/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.9857\n",
      "Epoch 198/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.9914\n",
      "Epoch 199/200\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.9886\n",
      "Epoch 200/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0190 - acc: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d36f82fa20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=200,batch_size=32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.47793102e-05],\n",
       "       [1.49403930e-01],\n",
       "       [1.77025795e-05],\n",
       "       [1.23381615e-05],\n",
       "       [9.38773155e-06],\n",
       "       [8.51452351e-05],\n",
       "       [9.99996781e-01],\n",
       "       [1.83880329e-05],\n",
       "       [1.00135803e-05],\n",
       "       [1.60479456e-01],\n",
       "       [1.59189850e-01],\n",
       "       [9.99995828e-01],\n",
       "       [7.79484689e-01],\n",
       "       [9.99965310e-01],\n",
       "       [9.99634862e-01],\n",
       "       [9.99996781e-01],\n",
       "       [4.61083263e-01],\n",
       "       [1.43448770e-01],\n",
       "       [9.47713852e-06],\n",
       "       [9.99901533e-01],\n",
       "       [1.74999237e-04],\n",
       "       [9.97345209e-01],\n",
       "       [9.99991357e-01],\n",
       "       [9.99894977e-01],\n",
       "       [7.80959427e-01],\n",
       "       [9.44553971e-01],\n",
       "       [2.76685953e-02],\n",
       "       [9.99996781e-01],\n",
       "       [9.99994040e-01],\n",
       "       [1.68985128e-01],\n",
       "       [6.78859711e-01],\n",
       "       [9.99949694e-01],\n",
       "       [6.00718856e-02],\n",
       "       [5.21729290e-01],\n",
       "       [9.99996185e-01],\n",
       "       [9.89437103e-06],\n",
       "       [9.34809446e-04],\n",
       "       [1.52984977e-01],\n",
       "       [9.53674316e-06],\n",
       "       [9.99996781e-01],\n",
       "       [9.98136878e-01],\n",
       "       [9.99518454e-01],\n",
       "       [9.76655006e-01],\n",
       "       [7.65919685e-05],\n",
       "       [1.02818012e-05],\n",
       "       [9.35792923e-06],\n",
       "       [6.61284924e-01],\n",
       "       [9.68564987e-01],\n",
       "       [6.49396718e-01],\n",
       "       [9.29832458e-06],\n",
       "       [9.99996245e-01],\n",
       "       [9.99996483e-01],\n",
       "       [1.01468861e-02],\n",
       "       [9.99996781e-01],\n",
       "       [1.37051702e-01],\n",
       "       [1.81394815e-03],\n",
       "       [3.84804487e-01],\n",
       "       [5.10718465e-01],\n",
       "       [9.35792923e-06],\n",
       "       [9.99880314e-01],\n",
       "       [3.91849041e-01],\n",
       "       [9.29832458e-06],\n",
       "       [9.99996305e-01],\n",
       "       [7.62075126e-01],\n",
       "       [9.99918163e-01],\n",
       "       [2.49445438e-05],\n",
       "       [1.60038471e-04],\n",
       "       [2.96763420e-01],\n",
       "       [1.78287476e-01],\n",
       "       [3.50773335e-05],\n",
       "       [9.99996543e-01],\n",
       "       [9.99986768e-01],\n",
       "       [1.65228665e-01],\n",
       "       [2.01719999e-03],\n",
       "       [9.47713852e-06],\n",
       "       [2.81155109e-04],\n",
       "       [9.99994218e-01],\n",
       "       [5.67813814e-02],\n",
       "       [9.23871994e-06],\n",
       "       [9.99844730e-01],\n",
       "       [9.99996185e-01],\n",
       "       [8.83643031e-01],\n",
       "       [9.99996662e-01],\n",
       "       [6.29044175e-02],\n",
       "       [9.99996662e-01],\n",
       "       [9.99996424e-01],\n",
       "       [9.96111155e-01],\n",
       "       [8.82966220e-02],\n",
       "       [9.99879003e-01],\n",
       "       [9.67179298e-01],\n",
       "       [9.29832458e-06],\n",
       "       [9.99980450e-01],\n",
       "       [2.61348486e-03],\n",
       "       [7.34031200e-05],\n",
       "       [8.98417532e-02],\n",
       "       [9.44733620e-06],\n",
       "       [1.69462264e-02],\n",
       "       [6.98660195e-01],\n",
       "       [1.33514404e-05],\n",
       "       [8.52925777e-02],\n",
       "       [6.15417957e-05],\n",
       "       [9.99985695e-01],\n",
       "       [9.99984622e-01],\n",
       "       [9.99996781e-01],\n",
       "       [9.53674316e-06],\n",
       "       [9.99246359e-01],\n",
       "       [8.12583327e-01],\n",
       "       [9.99996781e-01],\n",
       "       [1.28149986e-05],\n",
       "       [4.90595907e-01],\n",
       "       [9.68575478e-06],\n",
       "       [9.74989176e-01],\n",
       "       [1.34706497e-05],\n",
       "       [9.98279154e-01],\n",
       "       [9.98461485e-01],\n",
       "       [1.08337104e-02],\n",
       "       [9.99996245e-01],\n",
       "       [9.99996066e-01],\n",
       "       [9.99975085e-01],\n",
       "       [9.99339759e-01],\n",
       "       [4.67538834e-03],\n",
       "       [8.34206820e-01],\n",
       "       [9.99996424e-01],\n",
       "       [5.64890623e-01],\n",
       "       [9.29832458e-06],\n",
       "       [6.30468130e-04],\n",
       "       [3.79601002e-01],\n",
       "       [9.77516174e-06],\n",
       "       [1.50501728e-05],\n",
       "       [2.25859880e-03],\n",
       "       [3.21250081e-01],\n",
       "       [9.99930501e-01],\n",
       "       [1.04010105e-05],\n",
       "       [1.85881257e-01],\n",
       "       [9.99996722e-01],\n",
       "       [9.99987602e-01],\n",
       "       [3.94540071e-01],\n",
       "       [9.99989152e-01],\n",
       "       [9.99990702e-01],\n",
       "       [4.85921383e-01],\n",
       "       [9.38773155e-06],\n",
       "       [9.99994993e-01],\n",
       "       [2.62882948e-01],\n",
       "       [1.26957893e-05],\n",
       "       [8.91491771e-03],\n",
       "       [9.98973846e-05],\n",
       "       [1.01327896e-05],\n",
       "       [1.99850231e-01],\n",
       "       [6.05777860e-01],\n",
       "       [1.57976449e-02],\n",
       "       [9.99711931e-01],\n",
       "       [1.17719173e-05],\n",
       "       [9.99967098e-01],\n",
       "       [9.99660850e-01],\n",
       "       [8.22324991e-01],\n",
       "       [8.41796398e-04],\n",
       "       [9.99895155e-01],\n",
       "       [1.06144279e-01],\n",
       "       [9.99996424e-01],\n",
       "       [3.54030907e-01],\n",
       "       [2.31762975e-01],\n",
       "       [2.61068344e-04],\n",
       "       [4.90546227e-05],\n",
       "       [9.99866247e-01],\n",
       "       [5.46912670e-01],\n",
       "       [3.12328339e-05],\n",
       "       [9.64095831e-01],\n",
       "       [3.87430191e-05],\n",
       "       [9.44733620e-06],\n",
       "       [4.56000984e-01],\n",
       "       [2.04443932e-05],\n",
       "       [9.18565512e-01],\n",
       "       [9.99897897e-01],\n",
       "       [9.89735126e-05],\n",
       "       [1.05798244e-05],\n",
       "       [3.85863125e-01],\n",
       "       [9.29832458e-06],\n",
       "       [9.99996424e-01],\n",
       "       [9.71555710e-06],\n",
       "       [3.20064723e-02],\n",
       "       [9.99852240e-01],\n",
       "       [9.99995947e-01],\n",
       "       [9.29832458e-06],\n",
       "       [9.98908997e-01],\n",
       "       [9.68575478e-06],\n",
       "       [9.97254312e-01],\n",
       "       [2.53322721e-03],\n",
       "       [4.29540873e-04],\n",
       "       [9.86456871e-06],\n",
       "       [9.99996006e-01],\n",
       "       [9.38721418e-01],\n",
       "       [1.20401382e-05],\n",
       "       [2.64061272e-01],\n",
       "       [9.99996305e-01],\n",
       "       [2.63823926e-01],\n",
       "       [3.18285823e-03],\n",
       "       [9.99996781e-01],\n",
       "       [9.98661637e-01],\n",
       "       [1.01923943e-05],\n",
       "       [9.99331951e-01],\n",
       "       [9.99996662e-01],\n",
       "       [9.99996781e-01],\n",
       "       [9.95713830e-01],\n",
       "       [3.91629338e-03],\n",
       "       [5.38290203e-01],\n",
       "       [1.31371409e-01],\n",
       "       [9.99950469e-01],\n",
       "       [9.99995828e-01],\n",
       "       [9.65595245e-06],\n",
       "       [2.12490559e-05],\n",
       "       [9.99994874e-01],\n",
       "       [9.35792923e-06],\n",
       "       [1.02907419e-04],\n",
       "       [7.69371152e-01],\n",
       "       [9.99996781e-01],\n",
       "       [1.61826611e-05],\n",
       "       [3.42789382e-01],\n",
       "       [9.99992371e-01],\n",
       "       [2.91104198e-01],\n",
       "       [9.99996781e-01],\n",
       "       [1.57654285e-05],\n",
       "       [2.74479389e-05],\n",
       "       [8.24531913e-03],\n",
       "       [5.12897968e-05],\n",
       "       [9.99955416e-01],\n",
       "       [9.99996245e-01],\n",
       "       [4.04850245e-01],\n",
       "       [7.32193232e-01],\n",
       "       [9.99993861e-01],\n",
       "       [3.15519929e-01],\n",
       "       [2.69551933e-01],\n",
       "       [9.54688191e-01],\n",
       "       [1.39571786e-01],\n",
       "       [2.95098454e-01],\n",
       "       [9.99994159e-01],\n",
       "       [9.32812691e-06],\n",
       "       [1.03116035e-05],\n",
       "       [9.99996662e-01],\n",
       "       [1.23977661e-05],\n",
       "       [2.34603882e-04],\n",
       "       [4.56168652e-02],\n",
       "       [9.99989152e-01],\n",
       "       [9.90800738e-01],\n",
       "       [9.99963045e-01],\n",
       "       [1.55270100e-05],\n",
       "       [9.74948764e-01],\n",
       "       [9.99994993e-01],\n",
       "       [6.00937247e-01],\n",
       "       [3.29127908e-03],\n",
       "       [1.30940676e-02],\n",
       "       [1.87176466e-03],\n",
       "       [9.99995589e-01],\n",
       "       [9.99996305e-01],\n",
       "       [1.73870057e-01],\n",
       "       [9.99990463e-01],\n",
       "       [1.73479021e-02],\n",
       "       [2.24533826e-01],\n",
       "       [9.99996781e-01],\n",
       "       [5.45680523e-05],\n",
       "       [9.99984980e-01],\n",
       "       [1.69575214e-05],\n",
       "       [9.99996781e-01],\n",
       "       [7.10915804e-01],\n",
       "       [3.17984432e-01],\n",
       "       [2.57790089e-05],\n",
       "       [3.19975615e-03],\n",
       "       [1.88916922e-04],\n",
       "       [7.98106194e-05],\n",
       "       [9.32812691e-06],\n",
       "       [1.35302544e-05],\n",
       "       [2.28450507e-01],\n",
       "       [1.20463967e-03],\n",
       "       [9.99996483e-01],\n",
       "       [9.99905944e-01],\n",
       "       [1.53267384e-02],\n",
       "       [9.99996662e-01],\n",
       "       [1.03116035e-05],\n",
       "       [9.98613715e-01],\n",
       "       [6.90162182e-04],\n",
       "       [9.99945760e-01],\n",
       "       [5.38550258e-01],\n",
       "       [9.99995828e-01],\n",
       "       [2.28058428e-01],\n",
       "       [1.76727772e-05],\n",
       "       [6.55442476e-03],\n",
       "       [1.38074607e-01],\n",
       "       [9.96594667e-01],\n",
       "       [1.41387761e-01],\n",
       "       [9.99975204e-01],\n",
       "       [1.88787878e-02],\n",
       "       [4.84138727e-04],\n",
       "       [9.38773155e-06],\n",
       "       [3.41035455e-01],\n",
       "       [9.38773155e-06],\n",
       "       [1.20939523e-01],\n",
       "       [2.65719444e-01],\n",
       "       [2.61739879e-05],\n",
       "       [1.03319035e-05],\n",
       "       [1.32181933e-02],\n",
       "       [9.65744257e-01]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=(y_pred>0.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7133333333333334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mymodel.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
